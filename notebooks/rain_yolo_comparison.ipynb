{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rain Removal Video Processing with DIP R1/L2 and YOLO Tracking\n",
        "\n",
        "This notebook processes the Rain1.mp4 video through:\n",
        "1. DIP R1 pipeline (Rain Removal Enhancement)\n",
        "2. DIP R2 pipeline (Frequency-based Rain Attenuation)\n",
        "3. YOLO tracking on the original and both enhanced videos using `model.track`\n",
        "4. Comparison visualization across all three variants\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import shutil\n",
        "from IPython.display import Video, HTML, display\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Add project root to Python path\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from src.dip.processors import build_DIP_pipeline, run_DIP_pipeline\n",
        "from src.utils.video_utils import open_video, create_writer\n",
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ“¹ Video Display Notes\n",
        "\n",
        "**If videos don't display:**\n",
        "1. Make sure all cells above have been executed (variables must be defined)\n",
        "2. The videos use `embed=True` which embeds them in the notebook\n",
        "3. Large videos (40-79MB) may take 10-30 seconds to load\n",
        "4. If your browser freezes, clear outputs: `Cell > All Output > Clear`\n",
        "5. Re-run just the video display cell you want to see\n",
        "\n",
        "**Memory management:**\n",
        "- After viewing a video, clear its output to free memory\n",
        "- You can re-run the cell anytime to view it again\n",
        "- Alternatively, navigate to the `output/` folder and open MP4 files directly in a video player\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define paths\n",
        "project_root = \"/Users/pete/Desktop/253_Project\"\n",
        "input_video = os.path.join(project_root, \"videos/Rain/Rain1.mp4\")\n",
        "model_path = os.path.join(project_root, \"models/best.pt\")\n",
        "\n",
        "# Create output directory\n",
        "output_dir = os.path.join(project_root, \"output\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "print(f\"Input video: {input_video}\")\n",
        "print(f\"YOLO model: {model_path}\")\n",
        "print(f\"Output directory: {output_dir}\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input video: /Users/pete/Desktop/253_Project/videos/Rain/Rain1.mp4\n",
            "YOLO model: /Users/pete/Desktop/253_Project/models/best.pt\n",
            "Output directory: /Users/pete/Desktop/253_Project/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Apply DIP R1 and L2 Enhancements (Rain Removal)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def process_video_with_dip(input_video_path, output_video_path, dip_process_name):\n",
        "    \"\"\"Process a video through DIP pipeline and save the result.\"\"\"\n",
        "    print(f\"Processing video with DIP {dip_process_name}...\")\n",
        "    dip_function = build_DIP_pipeline(dip_process_name)\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {input_video_path}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"  Video info: {width}x{height} @ {fps:.2f} fps, {total_frames} frames\")\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        raise RuntimeError(f\"Failed to create writer: {output_video_path}\")\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        processed_frame = run_DIP_pipeline(frame, dip_function)\n",
        "        out.write(processed_frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"  Processed {frame_count}/{total_frames} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"DIP processing complete! Total frames: {frame_count}/{total_frames}\")\n",
        "\n",
        "# Process video with DIP R1 - output directly to MP4 for better smoothness\n",
        "dip_r1_mp4 = os.path.join(output_dir, \"Rain1_R1_processed.mp4\")\n",
        "process_video_with_dip(input_video, dip_r1_mp4, \"R1\")\n",
        "\n",
        "# Process video with DIP R2 for comparison\n",
        "dip_r2_mp4 = os.path.join(output_dir, \"Rain1_R2_processed.mp4\")\n",
        "process_video_with_dip(input_video, dip_r2_mp4, \"R2\")\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing video with DIP R1...\n",
            "  Video info: 1670x1080 @ 30.00 fps, 1530 frames\n",
            "  Processed 30/1530 frames...\n",
            "  Processed 60/1530 frames...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Process video with DIP R1 - output directly to MP4 for better smoothness\u001b[39;00m\n\u001b[32m     38\u001b[39m dip_r1_mp4 = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mRain1_R1_processed.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mprocess_video_with_dip\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_video\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdip_r1_mp4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mR1\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Process video with DIP R2 for comparison\u001b[39;00m\n\u001b[32m     42\u001b[39m dip_r2_mp4 = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mRain1_R2_processed.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprocess_video_with_dip\u001b[39m\u001b[34m(input_video_path, output_video_path, dip_process_name)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m processed_frame = \u001b[43mrun_DIP_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdip_function\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m out.write(processed_frame)\n\u001b[32m     29\u001b[39m frame_count += \u001b[32m1\u001b[39m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/253_Project/DIP/DIP.py:90\u001b[39m, in \u001b[36mrun_DIP_pipeline\u001b[39m\u001b[34m(frame, process_fn)\u001b[39m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m process_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m frame\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mprocess_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/253_Project/DIP/DIP.py:12\u001b[39m, in \u001b[36mprocess_r1\u001b[39m\u001b[34m(frame)\u001b[39m\n\u001b[32m     10\u001b[39m _RAIN_TEMPORAL_BUFFER.append(frame)\n\u001b[32m     11\u001b[39m stack = np.stack(_RAIN_TEMPORAL_BUFFER, axis=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m median = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmedian\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.astype(np.uint8)\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cv2.bilateralFilter(median, \u001b[32m9\u001b[39m, \u001b[32m75\u001b[39m, \u001b[32m75\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/DIPENV/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4001\u001b[39m, in \u001b[36mmedian\u001b[39m\u001b[34m(a, axis, out, overwrite_input, keepdims)\u001b[39m\n\u001b[32m   3916\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_median_dispatcher)\n\u001b[32m   3917\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmedian\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, overwrite_input=\u001b[38;5;28;01mFalse\u001b[39;00m, keepdims=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   3918\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   3919\u001b[39m \u001b[33;03m    Compute the median along the specified axis.\u001b[39;00m\n\u001b[32m   3920\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   3999\u001b[39m \n\u001b[32m   4000\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4001\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ureduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_median\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m                    \u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverwrite_input\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/DIPENV/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:3894\u001b[39m, in \u001b[36m_ureduce\u001b[39m\u001b[34m(a, func, keepdims, **kwargs)\u001b[39m\n\u001b[32m   3891\u001b[39m             index_out = (\u001b[32m0\u001b[39m, ) * nd\n\u001b[32m   3892\u001b[39m             kwargs[\u001b[33m'\u001b[39m\u001b[33mout\u001b[39m\u001b[33m'\u001b[39m] = out[(\u001b[38;5;28mEllipsis\u001b[39m, ) + index_out]\n\u001b[32m-> \u001b[39m\u001b[32m3894\u001b[39m r = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3897\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/DIPENV/lib/python3.12/site-packages/numpy/lib/_function_base_impl.py:4034\u001b[39m, in \u001b[36m_median\u001b[39m\u001b[34m(a, axis, out, overwrite_input)\u001b[39m\n\u001b[32m   4032\u001b[39m         part = a\n\u001b[32m   4033\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4034\u001b[39m     part = \u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4036\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m part.shape == ():\n\u001b[32m   4037\u001b[39m     \u001b[38;5;66;03m# make 0-D arrays work\u001b[39;00m\n\u001b[32m   4038\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m part.item()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/DIPENV/lib/python3.12/site-packages/numpy/_core/fromnumeric.py:868\u001b[39m, in \u001b[36mpartition\u001b[39m\u001b[34m(a, kth, axis, kind, order)\u001b[39m\n\u001b[32m    866\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    867\u001b[39m     a = asanyarray(a).copy(order=\u001b[33m\"\u001b[39m\u001b[33mK\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m868\u001b[39m \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    869\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run YOLO Detection on Original Video\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Performance optimization settings\n",
        "# Reduce imgsz for speed: 640 is ~2.25x faster than 960\n",
        "# Trade-off: Lower imgsz = faster but potentially less accurate\n",
        "imgsz = 640  # Options: 480 (fastest), 640 (balanced), 960 (most accurate)\n",
        "device = None  # Auto-detect: None, or specify \"cuda\", \"mps\", \"cpu\"\n",
        "half = False  # FP16 half precision (set True for GPU to speed up)\n",
        "\n",
        "# Load model once and reuse for all videos (faster than reloading)\n",
        "print(\"Loading YOLO model...\")\n",
        "model = YOLO(model_path)\n",
        "device_label = device if device is not None else \"auto\"\n",
        "print(f\"Model loaded. Device: {device_label}, imgsz: {imgsz}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def run_yolo_on_video(\n",
        "    input_video_path,\n",
        "    output_video_path,\n",
        "    model_path,\n",
        "    model=None,\n",
        "    imgsz=2560,\n",
        "    device=None,\n",
        "    half=False,\n",
        "):\n",
        "    \"\"\"Run YOLO tracking on a video and save annotated frames with counting info.\"\"\"\n",
        "\n",
        "    video_name = os.path.basename(input_video_path)\n",
        "    print(f\"Running YOLO tracking on {video_name}...\")\n",
        "\n",
        "    yolo_model = model if model is not None else YOLO(model_path)\n",
        "\n",
        "    runtime_device = device\n",
        "    if runtime_device is None:\n",
        "        import torch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            runtime_device = \"cuda\"\n",
        "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            runtime_device = \"mps\"\n",
        "        else:\n",
        "            runtime_device = \"cpu\"\n",
        "\n",
        "    print(f\"  Using device: {runtime_device}, imgsz: {imgsz}, half: {half}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {input_video_path}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"  Video info: {width}x{height} @ {fps:.2f} fps, {total_frames} frames\")\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        raise RuntimeError(f\"Failed to create writer: {output_video_path}\")\n",
        "\n",
        "    counting_line_y = int(height * 0.65)\n",
        "    counted_ids = set()\n",
        "    vehicle_count = 0\n",
        "    threshold = 55\n",
        "    frame_count = 0\n",
        "    total_detections = 0\n",
        "    frame_counts = {}\n",
        "\n",
        "    tracking_stream = yolo_model.track(\n",
        "        source=input_video_path,\n",
        "        imgsz=imgsz,\n",
        "        conf=0.01,\n",
        "        iou=0.4,\n",
        "        verbose=False,\n",
        "        max_det=300,\n",
        "        stream=True,\n",
        "        persist=True,\n",
        "        device=runtime_device,\n",
        "        half=half,\n",
        "    )\n",
        "\n",
        "    for frame_count, result in enumerate(tracking_stream, start=1):\n",
        "        annotated_frame = result.plot(line_width=2, conf=True)\n",
        "        boxes = result.boxes\n",
        "        num_detections = len(boxes) if boxes is not None else 0\n",
        "        total_detections += num_detections\n",
        "        frame_counts[frame_count] = num_detections\n",
        "\n",
        "        if boxes is not None and boxes.id is not None:\n",
        "            xyxy = boxes.xyxy.cpu().numpy()\n",
        "            track_ids = boxes.id.int().cpu().tolist()\n",
        "            for (x1, y1, x2, y2), track_id in zip(xyxy, track_ids):\n",
        "                cx = (x1 + x2) / 2.0\n",
        "                cy = (y1 + y2) / 2.0\n",
        "                if abs(cy - counting_line_y) < threshold and track_id not in counted_ids:\n",
        "                    counted_ids.add(track_id)\n",
        "                    vehicle_count += 1\n",
        "                cv2.circle(annotated_frame, (int(cx), int(cy)), 4, (0, 255, 255), -1)\n",
        "                cv2.putText(\n",
        "                    annotated_frame,\n",
        "                    f\"ID {track_id}\",\n",
        "                    (int(cx) - 20, int(cy) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.6,\n",
        "                    (0, 255, 255),\n",
        "                    2,\n",
        "                )\n",
        "\n",
        "        cv2.line(annotated_frame, (0, counting_line_y), (width, counting_line_y), (0, 255, 0), 3)\n",
        "        count_text = f\"Vehicle Count: {vehicle_count}\"\n",
        "        cv2.putText(annotated_frame, count_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        if frame_count % 30 == 0 or frame_count == total_frames:\n",
        "            print(f\"  Processed {frame_count}/{total_frames} frames...\")\n",
        "\n",
        "    out.release()\n",
        "    print(\n",
        "        f\"YOLO tracking complete! Frames: {frame_count}/{total_frames}, Total detections: {total_detections}\"\n",
        "    )\n",
        "    print(f\"Vehicles counted crossing line: {vehicle_count}\")\n",
        "    return vehicle_count, frame_counts\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run YOLO Detection on Original + DIP-Enhanced Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run YOLO tracking on original video - output directly to MP4\n",
        "original_yolo_mp4 = os.path.join(output_dir, \"Rain1_original_yolo.mp4\")\n",
        "\n",
        "original_count, original_frames = run_yolo_on_video(\n",
        "    input_video,\n",
        "    original_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": [],
      "id": "d6223984"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Run YOLO tracking on DIP R1 enhanced video - output directly to MP4\n",
        "processed_r1_yolo_mp4 = os.path.join(output_dir, \"Rain1_enhanced_yolo.mp4\")\n",
        "\n",
        "r1_count, r1_frames = run_yolo_on_video(\n",
        "    dip_r1_mp4,\n",
        "    processed_r1_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n",
        "\n",
        "# Run YOLO tracking on DIP R2 enhanced video - output directly to MP4\n",
        "processed_r2_yolo_mp4 = os.path.join(output_dir, \"Rain1_R2_processed_yolo.mp4\")\n",
        "\n",
        "r2_count, r2_frames = run_yolo_on_video(\n",
        "    dip_r2_mp4,\n",
        "    processed_r2_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vehicle Count Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting Results\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "ORIGINAL_COUNT = original_count\n",
        "print(\"=\" * 60)\n",
        "print(\"VEHICLE COUNTING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Original Video:       {ORIGINAL_COUNT} vehicles counted\")\n",
        "print(f\"DIP R1 Video:         {r1_count} vehicles counted\")\n",
        "print(f\"DIP R2 Video:         {r2_count} vehicles counted\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"R1 - Original:        {r1_count - original_count:+} vehicles\")\n",
        "print(f\"R2 - Original:        {r2_count - original_count:+} vehicles\")\n",
        "print(f\"R2 - R1:              {r2_count - r1_count:+} vehicles\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nThe DIP enhancements improve visibility in different ways, so review\")\n",
        "print(\"both outputs alongside the original to decide which tracking result\")\n",
        "print(\"aligns best with your downstream requirements.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Helper function for robust video display\n",
        "try:\n",
        "    import imageio_ffmpeg\n",
        "except ImportError:\n",
        "    print(\"Installing imageio-ffmpeg for video conversion...\")\n",
        "    %pip install -q imageio-ffmpeg\n",
        "    import imageio_ffmpeg\n",
        "\n",
        "def display_video_robust(video_path, width=960):\n",
        "    \"\"\"\n",
        "    Display video with fallback options if embedding fails.\n",
        "    Converts video to H.264 using ffmpeg (via imageio-ffmpeg) for browser compatibility.\n",
        "    \"\"\"\n",
        "    from IPython.display import Video, FileLink, display, HTML\n",
        "    import os\n",
        "    import subprocess\n",
        "    \n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"âŒ Video not found: {video_path}\")\n",
        "        print(\"   Run the processing cells above first.\")\n",
        "        return None\n",
        "\n",
        "    # Get ffmpeg executable from imageio-ffmpeg\n",
        "    ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "    \n",
        "    # Output path for the converted video (browser compatible)\n",
        "    base, ext = os.path.splitext(video_path)\n",
        "    converted_path = f\"{base}_h264.mp4\"\n",
        "    \n",
        "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
        "    \n",
        "    # Convert to H.264 using ffmpeg\n",
        "    # -y: overwrite output\n",
        "    # -loglevel panic: suppress output\n",
        "    # -vcodec libx264: use H.264 codec\n",
        "    # -pix_fmt yuv420p: ensure compatibility\n",
        "    # -acodec aac: audio codec (if audio exists)\n",
        "    cmd = [\n",
        "        ffmpeg_exe, \"-y\", \"-loglevel\", \"panic\",\n",
        "        \"-i\", video_path,\n",
        "        \"-vcodec\", \"libx264\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        \"-acodec\", \"aac\",\n",
        "        converted_path\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "        \n",
        "        if os.path.exists(converted_path):\n",
        "            file_size_mb = os.path.getsize(converted_path) / (1024*1024)\n",
        "            print(f\"ðŸ“¹ Displaying: {os.path.basename(converted_path)} ({file_size_mb:.1f} MB)\")\n",
        "            print(f\"   Loading... (may take 10-30 seconds for large files)\")\n",
        "            return Video(converted_path, embed=True, width=width, html_attributes=\"controls\")\n",
        "        else:\n",
        "            print(\"âš ï¸ Conversion failed (output file not created). displaying original...\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ffmpeg conversion failed: {e}\")\n",
        "        print(\"   Attempting to display original file...\")\n",
        "    \n",
        "    # Fallback to original\n",
        "    try:\n",
        "        file_size_mb = os.path.getsize(video_path) / (1024*1024)\n",
        "        print(f\"ðŸ“¹ Video: {os.path.basename(video_path)} ({file_size_mb:.1f} MB)\")\n",
        "        return Video(video_path, embed=True, width=width, html_attributes=\"controls\")\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸  Embedding failed: {e}\")\n",
        "        print(f\"   Alternative: Click to download/open externally:\")\n",
        "        display(FileLink(video_path))\n",
        "        return None\n",
        "\n",
        "print(\"âœ“ Helper function loaded. Use: display_video_robust(video_path)\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## YOLO Tracking Results with Vehicle Counting\n",
        "\n",
        "### YOLO on Original Video (with counting line and count overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display_video_robust(original_yolo_mp4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLO on DIP R1 Enhanced Video (with counting line and count overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display_video_robust(processed_r1_yolo_mp4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLO on DIP R2 Enhanced Video (with counting line and count overlay)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "display_video_robust(processed_r2_yolo_mp4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook processed the Rain1.mp4 video through:\n",
        "\n",
        "1. **DIP R1 Enhancement**: Applied Temporal median + Bilateral filtering in YCrCb color space\n",
        "2. **DIP R2 Enhancement**: Ran a second DIP variant to preserve edges and frequency-based rain attenuation\n",
        "3. **YOLO Tracking with `model.track()`**: Evaluated the original, DIP R1, and DIP R2 videos using the built-in tracker for persistent IDs\n",
        "4. **Vehicle Counting & Comparison**: Counted line crossings for each variant and compared the resulting counts and visuals\n",
        "\n",
        "### Key Features:\n",
        "- **Built-in Tracking**: Relied on `model.track()` for ID persistenceâ€”no custom centroid tracker required\n",
        "- **Better Codec**: Used mp4v codec directly instead of MJPG + ffmpeg conversion\n",
        "- **Frame Preservation**: Processed every frame without skipping to maintain smooth playback\n",
        "- **Proper FPS**: Ensured output videos maintain the source frame rate\n",
        "- **Vehicle Counting**: Simple line-crossing algorithm using YOLO track IDs so each vehicle is counted only once\n",
        "\n",
        "### How Vehicle Counting Works:\n",
        "1. A horizontal green line is drawn roughly 2/3 down each frame\n",
        "2. When a vehicle's center crosses this line, it is counted\n",
        "3. YOLO track IDs ensure each vehicle is counted only once (55-pixel threshold)\n",
        "4. The current count is displayed in the top-left corner of each frame\n",
        "\n",
        "Both DIP R1 and L2 improve visibility in low-light conditionsâ€”review their side-by-side outputs to decide which enhancement best serves your downstream analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Evaluation Metrics\n",
        "# Note: Ground truth available in data/ground_truth.csv\n",
        "# Rain1.mp4 ground truth: 47 vehicles\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VEHICLE COUNTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original Video:       {original_count} vehicles\")\n",
        "print(f\"DIP R1 Video:         {r1_count} vehicles\")\n",
        "print(f\"DIP R2 Video:         {r2_count} vehicles\")\n",
        "print(f\"Ground Truth:         47 vehicles\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nComparison with Ground Truth:\")\n",
        "print(f\"Original - GT:        {original_count - 47:+} vehicles\")\n",
        "print(f\"DIP R1 - GT:          {r1_count - 47:+} vehicles\")\n",
        "print(f\"DIP R2 - GT:          {r2_count - 47:+} vehicles\")\n",
        "print(\"=\"*60)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DIPENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}