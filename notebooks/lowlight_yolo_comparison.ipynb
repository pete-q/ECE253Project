{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Low Light Video Processing with DIP L1/L2 and YOLO Tracking\n",
        "\n",
        "This notebook processes the LL1.mp4 video through:\n",
        "1. DIP L1 pipeline (Low Light Enhancement)\n",
        "2. DIP L2 pipeline (Edge-preserving enhancement)\n",
        "3. YOLO tracking on the original and both enhanced videos using `model.track`\n",
        "4. Comparison visualization across all three variants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import shutil\n",
        "from IPython.display import Video, HTML, display\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Add project root to Python path (for importing src modules)\n",
        "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
        "if project_root not in sys.path:\n",
        "    sys.path.insert(0, project_root)\n",
        "\n",
        "from src.dip.processors import build_DIP_pipeline, run_DIP_pipeline\n",
        "from src.utils.video_utils import open_video, create_writer\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìπ Video Display Notes\n",
        "\n",
        "**If videos don't display:**\n",
        "1. Make sure all cells above have been executed (variables must be defined)\n",
        "2. The videos use `embed=True` which embeds them in the notebook\n",
        "3. Large videos (40-79MB) may take 10-30 seconds to load\n",
        "4. If your browser freezes, clear outputs: `Cell > All Output > Clear`\n",
        "5. Re-run just the video display cell you want to see\n",
        "\n",
        "**Memory management:**\n",
        "- After viewing a video, clear its output to free memory\n",
        "- You can re-run the cell anytime to view it again\n",
        "- Alternatively, navigate to the `output/` folder and open MP4 files directly in a video player\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup Paths\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input video: /Users/pete/Desktop/253_Project/videos/low_light/LL1_down.mp4\n",
            "YOLO model: yolov8x.pt\n",
            "Output directory: /Users/pete/Desktop/253_Project/output\n"
          ]
        }
      ],
      "source": [
        "### ========================================\n",
        "### CONFIGURATION: Update paths here\n",
        "### ========================================\n",
        "\n",
        "# Input video path\n",
        "input_video = os.path.join(project_root, \"videos/low_light/LL1_down.mp4\")\n",
        "\n",
        "# YOLO model path\n",
        "# Option 1: Use fine-tuned model from Farzad Nekouee\n",
        "model_path = os.path.join(project_root, \"models/best.pt\")\n",
        "# Option 2: Use pretrained YOLOv8 model (auto-downloads)\n",
        "# model_path = \"yolov8x.pt\"\n",
        "\n",
        "# Output directory for processed videos\n",
        "output_dir = os.path.join(project_root, \"output\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Display configuration\n",
        "print(\"=\"*60)\n",
        "print(\"CONFIGURATION\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Input video:    {input_video}\")\n",
        "print(f\"YOLO model:     {model_path}\")\n",
        "print(f\"Output folder:  {output_dir}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Apply DIP L1 and L2 Enhancements (Low Light)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing video with DIP L1...\n",
            "  Video info: 1920x1080 @ 29.97 fps, 1223 frames\n",
            "  Processed 30/1223 frames...\n",
            "  Processed 60/1223 frames...\n",
            "  Processed 90/1223 frames...\n",
            "  Processed 120/1223 frames...\n",
            "  Processed 150/1223 frames...\n",
            "  Processed 180/1223 frames...\n",
            "  Processed 210/1223 frames...\n",
            "  Processed 240/1223 frames...\n",
            "  Processed 270/1223 frames...\n",
            "  Processed 300/1223 frames...\n",
            "  Processed 330/1223 frames...\n",
            "  Processed 360/1223 frames...\n",
            "  Processed 390/1223 frames...\n",
            "  Processed 420/1223 frames...\n",
            "  Processed 450/1223 frames...\n",
            "  Processed 480/1223 frames...\n",
            "  Processed 510/1223 frames...\n",
            "  Processed 540/1223 frames...\n",
            "  Processed 570/1223 frames...\n",
            "  Processed 600/1223 frames...\n",
            "  Processed 630/1223 frames...\n",
            "  Processed 660/1223 frames...\n",
            "  Processed 690/1223 frames...\n",
            "  Processed 720/1223 frames...\n",
            "  Processed 750/1223 frames...\n",
            "  Processed 780/1223 frames...\n",
            "  Processed 810/1223 frames...\n",
            "  Processed 840/1223 frames...\n",
            "  Processed 870/1223 frames...\n",
            "  Processed 900/1223 frames...\n",
            "  Processed 930/1223 frames...\n",
            "  Processed 960/1223 frames...\n",
            "  Processed 990/1223 frames...\n",
            "  Processed 1020/1223 frames...\n",
            "  Processed 1050/1223 frames...\n",
            "  Processed 1080/1223 frames...\n",
            "  Processed 1110/1223 frames...\n",
            "  Processed 1140/1223 frames...\n",
            "  Processed 1170/1223 frames...\n",
            "  Processed 1200/1223 frames...\n",
            "DIP processing complete! Total frames: 1223/1223\n",
            "Processing video with DIP L2...\n",
            "  Video info: 1920x1080 @ 29.97 fps, 1223 frames\n",
            "  Processed 30/1223 frames...\n",
            "  Processed 60/1223 frames...\n",
            "  Processed 90/1223 frames...\n",
            "  Processed 120/1223 frames...\n",
            "  Processed 150/1223 frames...\n",
            "  Processed 180/1223 frames...\n",
            "  Processed 210/1223 frames...\n",
            "  Processed 240/1223 frames...\n",
            "  Processed 270/1223 frames...\n",
            "  Processed 300/1223 frames...\n",
            "  Processed 330/1223 frames...\n",
            "  Processed 360/1223 frames...\n",
            "  Processed 390/1223 frames...\n",
            "  Processed 420/1223 frames...\n",
            "  Processed 450/1223 frames...\n",
            "  Processed 480/1223 frames...\n",
            "  Processed 510/1223 frames...\n",
            "  Processed 540/1223 frames...\n",
            "  Processed 570/1223 frames...\n",
            "  Processed 600/1223 frames...\n",
            "  Processed 630/1223 frames...\n",
            "  Processed 660/1223 frames...\n",
            "  Processed 690/1223 frames...\n",
            "  Processed 720/1223 frames...\n",
            "  Processed 750/1223 frames...\n",
            "  Processed 780/1223 frames...\n",
            "  Processed 810/1223 frames...\n",
            "  Processed 840/1223 frames...\n",
            "  Processed 870/1223 frames...\n",
            "  Processed 900/1223 frames...\n",
            "  Processed 930/1223 frames...\n",
            "  Processed 960/1223 frames...\n",
            "  Processed 990/1223 frames...\n",
            "  Processed 1020/1223 frames...\n",
            "  Processed 1050/1223 frames...\n",
            "  Processed 1080/1223 frames...\n",
            "  Processed 1110/1223 frames...\n",
            "  Processed 1140/1223 frames...\n",
            "  Processed 1170/1223 frames...\n",
            "  Processed 1200/1223 frames...\n",
            "DIP processing complete! Total frames: 1223/1223\n"
          ]
        }
      ],
      "source": [
        "def process_video_with_dip(input_video_path, output_video_path, dip_process_name):\n",
        "    \"\"\"Process a video through DIP pipeline and save the result.\"\"\"\n",
        "    print(f\"Processing video with DIP {dip_process_name}...\")\n",
        "    dip_function = build_DIP_pipeline(dip_process_name)\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {input_video_path}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    print(f\"  Video info: {width}x{height} @ {fps:.2f} fps, {total_frames} frames\")\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        raise RuntimeError(f\"Failed to create writer: {output_video_path}\")\n",
        "\n",
        "    frame_count = 0\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "        processed_frame = run_DIP_pipeline(frame, dip_function)\n",
        "        out.write(processed_frame)\n",
        "        frame_count += 1\n",
        "        if frame_count % 30 == 0:\n",
        "            print(f\"  Processed {frame_count}/{total_frames} frames...\")\n",
        "\n",
        "    cap.release()\n",
        "    out.release()\n",
        "    print(f\"DIP processing complete! Total frames: {frame_count}/{total_frames}\")\n",
        "\n",
        "# Process video with DIP L1 - output directly to MP4 for better smoothness\n",
        "dip_enhanced_mp4 = os.path.join(output_dir, \"LL1_L1_enhanced.mp4\")\n",
        "process_video_with_dip(input_video, dip_enhanced_mp4, \"L1\")\n",
        "\n",
        "# Process video with DIP L2 for comparison\n",
        "dip_enhanced_l2_mp4 = os.path.join(output_dir, \"LL1_L2_enhanced.mp4\")\n",
        "process_video_with_dip(input_video, dip_enhanced_l2_mp4, \"L2\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Run YOLO Detection on Original Video\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading YOLO model...\n",
            "Model loaded. Device: auto, imgsz: 1280\n"
          ]
        }
      ],
      "source": [
        "# Performance optimization settings\n",
        "# Reduce imgsz for speed: 640 is ~2.25x faster than 960\n",
        "# Trade-off: Lower imgsz = faster but potentially less accurate\n",
        "imgsz = 1280  # Options: 480 (fastest), 640 (balanced), 960 (most accurate)\n",
        "device = None  # Auto-detect: None, or specify \"cuda\", \"mps\", \"cpu\"\n",
        "half = False  # FP16 half precision (set True for GPU to speed up)\n",
        "\n",
        "# Load model once and reuse for all videos (faster than reloading)\n",
        "print(\"Loading YOLO model...\")\n",
        "model = YOLO(model_path)\n",
        "device_label = device if device is not None else \"auto\"\n",
        "print(f\"Model loaded. Device: {device_label}, imgsz: {imgsz}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_yolo_on_video(\n",
        "    input_video_path,\n",
        "    output_video_path,\n",
        "    model_path,\n",
        "    model=None,\n",
        "    imgsz=1280,\n",
        "    device=None,\n",
        "    half=False,\n",
        "):\n",
        "    \"\"\"Run YOLO tracking on a video and save annotated frames with counting info.\"\"\"\n",
        "\n",
        "    video_name = os.path.basename(input_video_path)\n",
        "    print(f\"Running YOLO tracking on {video_name}...\")\n",
        "\n",
        "    yolo_model = model if model is not None else YOLO(model_path)\n",
        "\n",
        "    runtime_device = device\n",
        "    if runtime_device is None:\n",
        "        import torch\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            runtime_device = \"cuda\"\n",
        "        elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "            runtime_device = \"mps\"\n",
        "        else:\n",
        "            runtime_device = \"cpu\"\n",
        "\n",
        "    print(f\"  Using device: {runtime_device}, imgsz: {imgsz}, half: {half}\")\n",
        "\n",
        "    cap = cv2.VideoCapture(input_video_path)\n",
        "    if not cap.isOpened():\n",
        "        raise RuntimeError(f\"Failed to open video: {input_video_path}\")\n",
        "\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"  Video info: {width}x{height} @ {fps:.2f} fps, {total_frames} frames\")\n",
        "\n",
        "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "    if not out.isOpened():\n",
        "        raise RuntimeError(f\"Failed to create writer: {output_video_path}\")\n",
        "\n",
        "    counting_line_y = int(height * 0.65)\n",
        "    counted_ids = set()\n",
        "    vehicle_count = 0\n",
        "    threshold = 55\n",
        "    frame_count = 0\n",
        "    total_detections = 0\n",
        "    frame_counts = {}\n",
        "\n",
        "    tracking_stream = yolo_model.track(\n",
        "        source=input_video_path,\n",
        "        imgsz=imgsz,\n",
        "        conf=0.01,\n",
        "        iou=0.4,\n",
        "        verbose=False,\n",
        "        max_det=300,\n",
        "        stream=True,\n",
        "        persist=True,\n",
        "        device=runtime_device,\n",
        "        half=half,\n",
        "    )\n",
        "\n",
        "    for frame_count, result in enumerate(tracking_stream, start=1):\n",
        "        annotated_frame = result.plot(line_width=2, conf=True)\n",
        "        boxes = result.boxes\n",
        "        num_detections = len(boxes) if boxes is not None else 0\n",
        "        total_detections += num_detections\n",
        "        frame_counts[frame_count] = num_detections\n",
        "\n",
        "        if boxes is not None and boxes.id is not None:\n",
        "            xyxy = boxes.xyxy.cpu().numpy()\n",
        "            track_ids = boxes.id.int().cpu().tolist()\n",
        "            for (x1, y1, x2, y2), track_id in zip(xyxy, track_ids):\n",
        "                cx = (x1 + x2) / 2.0\n",
        "                cy = (y1 + y2) / 2.0\n",
        "                if abs(cy - counting_line_y) < threshold and track_id not in counted_ids:\n",
        "                    counted_ids.add(track_id)\n",
        "                    vehicle_count += 1\n",
        "                cv2.circle(annotated_frame, (int(cx), int(cy)), 4, (0, 255, 255), -1)\n",
        "                cv2.putText(\n",
        "                    annotated_frame,\n",
        "                    f\"ID {track_id}\",\n",
        "                    (int(cx) - 20, int(cy) - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.6,\n",
        "                    (0, 255, 255),\n",
        "                    2,\n",
        "                )\n",
        "\n",
        "        cv2.line(annotated_frame, (0, counting_line_y), (width, counting_line_y), (0, 255, 0), 3)\n",
        "        count_text = f\"Vehicle Count: {vehicle_count}\"\n",
        "        cv2.putText(annotated_frame, count_text, (10, 40), cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)\n",
        "        out.write(annotated_frame)\n",
        "\n",
        "        if frame_count % 30 == 0 or frame_count == total_frames:\n",
        "            print(f\"  Processed {frame_count}/{total_frames} frames...\")\n",
        "\n",
        "    out.release()\n",
        "    print(\n",
        "        f\"YOLO tracking complete! Frames: {frame_count}/{total_frames}, Total detections: {total_detections}\"\n",
        "    )\n",
        "    print(f\"Vehicles counted crossing line: {vehicle_count}\")\n",
        "    return vehicle_count, frame_counts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Run YOLO Detection on Original + DIP-Enhanced Videos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d6223984",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running YOLO tracking on LL1_down.mp4...\n",
            "  Using device: mps, imgsz: 1280, half: False\n",
            "  Video info: 1920x1080 @ 29.97 fps, 1223 frames\n",
            "  Processed 30/1223 frames...\n",
            "  Processed 60/1223 frames...\n",
            "  Processed 90/1223 frames...\n",
            "  Processed 120/1223 frames...\n",
            "  Processed 150/1223 frames...\n",
            "  Processed 180/1223 frames...\n",
            "  Processed 210/1223 frames...\n",
            "  Processed 240/1223 frames...\n",
            "  Processed 270/1223 frames...\n",
            "  Processed 300/1223 frames...\n",
            "  Processed 330/1223 frames...\n",
            "  Processed 360/1223 frames...\n",
            "  Processed 390/1223 frames...\n",
            "  Processed 420/1223 frames...\n",
            "  Processed 450/1223 frames...\n",
            "  Processed 480/1223 frames...\n",
            "  Processed 510/1223 frames...\n",
            "  Processed 540/1223 frames...\n",
            "  Processed 570/1223 frames...\n",
            "  Processed 600/1223 frames...\n",
            "  Processed 630/1223 frames...\n",
            "  Processed 660/1223 frames...\n",
            "  Processed 690/1223 frames...\n",
            "  Processed 720/1223 frames...\n",
            "  Processed 750/1223 frames...\n",
            "  Processed 780/1223 frames...\n",
            "  Processed 810/1223 frames...\n",
            "  Processed 840/1223 frames...\n",
            "  Processed 870/1223 frames...\n",
            "  Processed 900/1223 frames...\n",
            "  Processed 930/1223 frames...\n",
            "  Processed 960/1223 frames...\n",
            "  Processed 990/1223 frames...\n",
            "  Processed 1020/1223 frames...\n",
            "  Processed 1050/1223 frames...\n",
            "  Processed 1080/1223 frames...\n",
            "  Processed 1110/1223 frames...\n",
            "  Processed 1140/1223 frames...\n",
            "  Processed 1170/1223 frames...\n",
            "  Processed 1200/1223 frames...\n",
            "  Processed 1223/1223 frames...\n",
            "YOLO tracking complete! Frames: 1223/1223, Total detections: 11788\n",
            "Vehicles counted crossing line: 37\n"
          ]
        }
      ],
      "source": [
        "# Run YOLO tracking on original video - output directly to MP4\n",
        "original_yolo_mp4 = os.path.join(output_dir, \"LL1_original_yolo.mp4\")\n",
        "\n",
        "original_count, original_frames = run_yolo_on_video(\n",
        "    input_video,\n",
        "    original_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running YOLO tracking on LL1_L1_enhanced.mp4...\n",
            "  Using device: mps, imgsz: 1280, half: False\n",
            "  Video info: 1920x1080 @ 29.97 fps, 1223 frames\n",
            "  Processed 30/1223 frames...\n",
            "  Processed 60/1223 frames...\n",
            "  Processed 90/1223 frames...\n",
            "  Processed 120/1223 frames...\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run YOLO tracking on DIP L1 enhanced video - output directly to MP4\u001b[39;00m\n\u001b[32m      2\u001b[39m enhanced_yolo_mp4 = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mLL1_enhanced_yolo.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m enhanced_count, enhanced_frames = \u001b[43mrun_yolo_on_video\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdip_enhanced_mp4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43menhanced_yolo_mp4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhalf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run YOLO tracking on DIP L2 enhanced video - output directly to MP4\u001b[39;00m\n\u001b[32m     15\u001b[39m enhanced_l2_yolo_mp4 = os.path.join(output_dir, \u001b[33m\"\u001b[39m\u001b[33mLL1_L2_enhanced_yolo.mp4\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 98\u001b[39m, in \u001b[36mrun_yolo_on_video\u001b[39m\u001b[34m(input_video_path, output_video_path, model_path, model, imgsz, device, half)\u001b[39m\n\u001b[32m     96\u001b[39m count_text = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVehicle Count: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvehicle_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     97\u001b[39m cv2.putText(annotated_frame, count_text, (\u001b[32m10\u001b[39m, \u001b[32m40\u001b[39m), cv2.FONT_HERSHEY_SIMPLEX, \u001b[32m1.2\u001b[39m, (\u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m0\u001b[39m), \u001b[32m3\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[43mout\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mannotated_frame\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m frame_count % \u001b[32m30\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m frame_count == total_frames:\n\u001b[32m    101\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mframe_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_frames\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m frames...\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Run YOLO tracking on DIP L1 enhanced video - output directly to MP4\n",
        "enhanced_yolo_mp4 = os.path.join(output_dir, \"LL1_enhanced_yolo.mp4\")\n",
        "\n",
        "enhanced_count, enhanced_frames = run_yolo_on_video(\n",
        "    dip_enhanced_mp4,\n",
        "    enhanced_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n",
        "\n",
        "# Run YOLO tracking on DIP L2 enhanced video - output directly to MP4\n",
        "enhanced_l2_yolo_mp4 = os.path.join(output_dir, \"LL1_L2_enhanced_yolo.mp4\")\n",
        "\n",
        "enhanced_l2_count, enhanced_l2_frames = run_yolo_on_video(\n",
        "    dip_enhanced_l2_mp4,\n",
        "    enhanced_l2_yolo_mp4,\n",
        "    model_path,\n",
        "    model=model,\n",
        "    imgsz=imgsz,\n",
        "    device=device,\n",
        "    half=half,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Vehicle Count Comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Counting Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\" * 60)\n",
        "print(\"VEHICLE COUNTING RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Original Video:       {original_count} vehicles counted\")\n",
        "print(f\"DIP L1 Video:         {enhanced_count} vehicles counted\")\n",
        "print(f\"DIP L2 Video:         {enhanced_l2_count} vehicles counted\")\n",
        "print(\"-\" * 60)\n",
        "print(f\"L1 - Original:        {enhanced_count - original_count:+} vehicles\")\n",
        "print(f\"L2 - Original:        {enhanced_l2_count - original_count:+} vehicles\")\n",
        "print(f\"L2 - L1:              {enhanced_l2_count - enhanced_count:+} vehicles\")\n",
        "print(\"=\" * 60)\n",
        "print(\"\\nThe DIP enhancements improve visibility in different ways, so review\")\n",
        "print(\"both outputs alongside the original to decide which tracking result\")\n",
        "print(\"aligns best with your downstream requirements.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper function for robust video display\n",
        "try:\n",
        "    import imageio_ffmpeg\n",
        "except ImportError:\n",
        "    print(\"Installing imageio-ffmpeg for video conversion...\")\n",
        "    %pip install -q imageio-ffmpeg\n",
        "    import imageio_ffmpeg\n",
        "\n",
        "def display_video_robust(video_path, width=960):\n",
        "    \"\"\"\n",
        "    Display video with fallback options if embedding fails.\n",
        "    Converts video to H.264 using ffmpeg (via imageio-ffmpeg) for browser compatibility.\n",
        "    \"\"\"\n",
        "    from IPython.display import Video, FileLink, display, HTML\n",
        "    import os\n",
        "    import subprocess\n",
        "    \n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"‚ùå Video not found: {video_path}\")\n",
        "        print(\"   Run the processing cells above first.\")\n",
        "        return None\n",
        "\n",
        "    # Get ffmpeg executable from imageio-ffmpeg\n",
        "    ffmpeg_exe = imageio_ffmpeg.get_ffmpeg_exe()\n",
        "    \n",
        "    # Output path for the converted video (browser compatible)\n",
        "    base, ext = os.path.splitext(video_path)\n",
        "    converted_path = f\"{base}_h264.mp4\"\n",
        "    \n",
        "    print(f\"Processing video: {os.path.basename(video_path)}\")\n",
        "    \n",
        "    # Convert to H.264 using ffmpeg\n",
        "    # -y: overwrite output\n",
        "    # -loglevel panic: suppress output\n",
        "    # -vcodec libx264: use H.264 codec\n",
        "    # -pix_fmt yuv420p: ensure compatibility\n",
        "    # -acodec aac: audio codec (if audio exists)\n",
        "    cmd = [\n",
        "        ffmpeg_exe, \"-y\", \"-loglevel\", \"panic\",\n",
        "        \"-i\", video_path,\n",
        "        \"-vcodec\", \"libx264\",\n",
        "        \"-pix_fmt\", \"yuv420p\",\n",
        "        \"-acodec\", \"aac\",\n",
        "        converted_path\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        subprocess.run(cmd, check=True)\n",
        "        \n",
        "        if os.path.exists(converted_path):\n",
        "            file_size_mb = os.path.getsize(converted_path) / (1024*1024)\n",
        "            print(f\"üìπ Displaying: {os.path.basename(converted_path)} ({file_size_mb:.1f} MB)\")\n",
        "            print(f\"   Loading... (may take 10-30 seconds for large files)\")\n",
        "            return Video(converted_path, embed=True, width=width, html_attributes=\"controls\")\n",
        "        else:\n",
        "            print(\"‚ö†Ô∏è Conversion failed (output file not created). displaying original...\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è ffmpeg conversion failed: {e}\")\n",
        "        print(\"   Attempting to display original file...\")\n",
        "    \n",
        "    # Fallback to original\n",
        "    try:\n",
        "        file_size_mb = os.path.getsize(video_path) / (1024*1024)\n",
        "        print(f\"üìπ Video: {os.path.basename(video_path)} ({file_size_mb:.1f} MB)\")\n",
        "        return Video(video_path, embed=True, width=width, html_attributes=\"controls\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è  Embedding failed: {e}\")\n",
        "        print(f\"   Alternative: Click to download/open externally:\")\n",
        "        display(FileLink(video_path))\n",
        "        return None\n",
        "\n",
        "print(\"‚úì Helper function loaded. Use: display_video_robust(video_path)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## YOLO Tracking Results with Vehicle Counting\n",
        "\n",
        "### YOLO on Original Video (with counting line and count overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_video_robust(original_yolo_mp4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLO on DIP L1 Enhanced Video (with counting line and count overlay)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_video_robust(enhanced_yolo_mp4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### YOLO on DIP L2 Enhanced Video (with counting line and count overlay)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "display_video_robust(enhanced_l2_yolo_mp4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook processed the LL1.mp4 video through:\n",
        "\n",
        "1. **DIP L1 Enhancement**: Applied CLAHE-based low-light enhancement in YCrCb color space\n",
        "2. **DIP L2 Enhancement**: Ran a second DIP variant to preserve edges and boost contrast differently\n",
        "3. **YOLO Tracking with `model.track()`**: Evaluated the original, DIP L1, and DIP L2 videos using the built-in tracker for persistent IDs\n",
        "4. **Vehicle Counting & Comparison**: Counted line crossings for each variant and compared the resulting counts and visuals\n",
        "\n",
        "### Key Features:\n",
        "- **Built-in Tracking**: Relied on `model.track()` for ID persistence‚Äîno custom centroid tracker required\n",
        "- **Better Codec**: Used mp4v codec directly instead of MJPG + ffmpeg conversion\n",
        "- **Frame Preservation**: Processed every frame without skipping to maintain smooth playback\n",
        "- **Proper FPS**: Ensured output videos maintain the source frame rate\n",
        "- **Vehicle Counting**: Simple line-crossing algorithm using YOLO track IDs so each vehicle is counted only once\n",
        "\n",
        "### How Vehicle Counting Works:\n",
        "1. A horizontal green line is drawn roughly 2/3 down each frame\n",
        "2. When a vehicle's center crosses this line, it is counted\n",
        "3. YOLO track IDs ensure each vehicle is counted only once (55-pixel threshold)\n",
        "4. The current count is displayed in the top-left corner of each frame\n",
        "\n",
        "Both DIP L1 and L2 improve visibility in low-light conditions‚Äîreview their side-by-side outputs to decide which enhancement best serves your downstream analytics.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluation Metrics\n",
        "# Note: Ground truth available in data/ground_truth.csv\n",
        "# LL1.mp4 ground truth: 37 vehicles\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"VEHICLE COUNTING RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Original Video:       {original_count} vehicles\")\n",
        "print(f\"DIP L1 Video:         {enhanced_count} vehicles\")\n",
        "print(f\"DIP L2 Video:         {enhanced_l2_count} vehicles\")\n",
        "print(f\"Ground Truth:         37 vehicles\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nComparison with Ground Truth:\")\n",
        "print(f\"Original - GT:        {original_count - 37:+} vehicles\")\n",
        "print(f\"DIP L1 - GT:          {enhanced_count - 37:+} vehicles\")\n",
        "print(f\"DIP L2 - GT:          {enhanced_l2_count - 37:+} vehicles\")\n",
        "print(\"=\"*60)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "DIPENV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
